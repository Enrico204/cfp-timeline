name: Update scraping

on:
  schedule:
    # 7am UTC = 9am CET, 3am EDT, 0am PDT − should be light hours globally for WikiCFP.
    - cron: '0 7 * * *'
  workflow_dispatch:


jobs:
  update:
    name: Update scraping
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v1

    - name: Install dependencies
      run: |
        python3 -m pip install -r ./requirements.txt
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git config --global user.name "github-actions[bot]"

    - name: Do the scraping
      run: |
        # Expected ~3200 requests, ~4h30 at the wikicfp-requested 5s interval, <1h with 1s interval
        # Limit for github actions is 6h, so let’s got with 5s delay
        (( $RANDOM < 32768 / 30 )) && python3 ./fetch_core+cfp.py --quiet --no-cache --delay 5 update-core
        python3 ./fetch_core+cfp.py --quiet --no-cache --delay 5 update-cfp

    - name: Push to GitHub
      run: |
        git add core.csv cfp.json parsing_errors.txt
        git commit -m "Update scraping $(date --rfc-3339=date)"
        git push https://${TOKEN}@github.com/${GITHUB_REPOSITORY} HEAD:master
      env:
        TOKEN: ${{ secrets.CORE_CFP_PUSHER }}
